# Bits, FLOPS, and Watts: A Systems-Level Perspective of Scaling LLMs

## Introduction
Scaling Large Language Models (LLMs) goes beyond simply increasing parameter counts. It emerges from a complex interplay of hardware, software, and algorithmic optimizations. These deep neural networks operate in high-dimensional vector spaces with complex loss landscapes and scaling power laws, where learning unfolds along intricate manifolds that often defy human intuition. Yet their existence is tethered to silicon and electricity — bound by thermodynamics, semiconductor physics, and the finite resources of our four-dimensional world. Ultimately, the mathematics of LLMs is bound by the very physics of silicon.

To understand their evolution, we must trace the journey of neural architectures from simple single-layer perceptrons to today’s trillion-parameter Transformers. At every stage, algorithmic breakthroughs and hardware advancements have propelled each other forward. As transistor scaling, memory bandwidth, and parallelism improved, each wave of innovation pushed the boundaries of what neural networks could achieve, reinforcing the deep synergy between software and silicon. Today’s relentless drive to build ever-larger LLMs stands upon that same foundation, revealing that the future of LLM scaling depends not on software or hardware alone, but on their careful co-design, emphasizing the need for a systems-level perspective on scaling. 

Adopting such a perspective, the triad of Bits, FLOPS, and Watts provides a systems-level lens to understand the scaling dynamics of LLMs. Information capacity, measured in bits, is determined by parameter count, weight precision, model architecture, and data entropy, all of which collectively define a model’s representational power. Computational throughput, measured in FLOPS, directly influences both training and inference speed, shaping the model’s efficiency and responsiveness. And watts measure energy consumption, highlighting the growing power costs and associated economic implications of scaling up LLMs. Each bit, woven in LLMs vast network of weights, biases and activations, demands energy to store and process, unfolding into a cascade of FLOPS orchestrated by increasingly complex and power-hungry hardware. At the same time, the silent but absolute arbiter of watts sets the ultimate boundary for this computational symphony, reminding us that unlimited scaling remains tethered to the thermodynamic constraints of our physical universe. That limit is not arbitrary. Landauer’s principle, a cornerstone of information theory, states that erasing even a single bit of information carries an irreducible energy cost. While we are orders of magnitude above this limit, it highlights the intimate connection between information, computation, and thermodynamics. As models scale to trillions of parameters, each bit processed accumulates a thermodynamic debt that must be repaid through heat dissipation. Absent rigorous hardware software co-design and efficient energy management, the trillion-parameter LLM era risks sliding into thermodynamic insolvency.

This challenge underscores the intricate relationship between information capacity, computational throughput, and energy consumption. Optimizing these three dimensions in tandem is crucial. It spurs the development of novel architectures, new algorithms, and hardware innovations that cater to all three resource demands holistically. This two-part series takes a deep dive into these challenges from both software and hardware standpoints.

## Part 1 Preview: The Laws of Scaling
We will begin [Part 1](Bits,%20FLOPS,%20and%20Watts_%20A%20Systems-Level%20Perspective%20of%20Scaling%20LLMs.pdf) by tracing the evolution of LLMs, highlighting the critical role of software and hardware co-design in shaping deep neural networks. From there, we develop an intuitive understanding of neural scaling laws and their intricate relationships with model performance, compute, data, model size, and the emergence of intelligence. We will also examine how these scaling laws extend to inference-time compute in models like o1/o3 and DeepSeek-R1, and how they relate to advanced reasoning capabilities such as Chain-of-Thought (CoT). Throughout this exploration, we introduce pivotal theoretical concepts such as irreducible entropy, the efficient-compute frontier, emergent abilities, and the manifold hypothesis. These concepts will illuminate both the extraordinary potential and the fundamental limitations of LLMs.

Next, we conduct a thorough analysis of the Transformer architecture, uncovering its theoretical underpinnings and the mechanisms by which it generates language with unreasonable effectiveness. We explore why Transformers serve as the backbone of LLMs and how its design choices influence scalability, especially the significant demands placed on the underlying compute infrastructure by Transformers inherent algorithmic complexity. We will focus on topics such as the arithmetic intensity of self-attention, the space complexity of the KV cache, the overhead of non-linear operations, and the role of sparsity in attention. This exploration will reveal how this complexity necessitates increasing amounts of memory (bits), computational throughput (FLOPS), and, most critically, leads to the unsustainable cost of power (watts) required to train and run today’s trillion-parameter LLMs.

By the conclusion of [Part 1](Bits,%20FLOPS,%20and%20Watts_%20A%20Systems-Level%20Perspective%20of%20Scaling%20LLMs.pdf), we establish a central tension at the heart of LLM advancement: while neural scaling laws push us inexorably toward ever-larger models in our quest for Artificial General Intelligence (AGI), the algorithmic complexity of Transformers creates physical bottlenecks that cannot be overcome through software optimization alone. This comprehensive framework sets the stage for Part 2, where we will analyze the critical triad of bits, FLOPS, and watts from a systems-level perspective, exploring potential pathways forward in addressing these challenges.

## Part 2 Preview: The Walls of Silicon
In [Part 2](Bits,%20FLOPS,%20and%20Watts_%20A%20Systems-Level%20Perspective%20of%20Scaling%20LLMs.pdf), we explore how the fundamental interplay between bits, FLOPS, and watts erects three critical barriers in GPU-based accelerated computing: the Memory Wall, Power Wall, and Interconnect Wall. These barriers aren’t merely engineering challenges, they represent fundamental physical principles governing semiconductor technology and information processing. At their core, these walls arise because information in physical form requires space (memory), energy (power), and efficient pathways (interconnects) — all finite resources constrained by transistor density, heat dissipation limits, and the speed of light. In silicon-based computers operating at the nanometer scale, electrons simultaneously serve as both carriers of information and energy, embodying bits and watts within a single quantum entity.

Our exploration begins with an examination of how modern GPUs operate, focusing on execution flow, tensor cores, and warp schedulers. We’ll analyze a kernel performing WMMA (warp matrix multiply-accumulate) operation using [low-level PTX ISA](WMMA_kernel/wmma_kernel_demo.ptx), revealing how the “Memory Wall” naturally emerges from the classical Von Neumann architecture. This wall represents the growing disparity between computational capability and memory performance — a gap that widens as processing power increases. To understand this challenge more deeply, we’ll examine hierarchical memory structures, from on-chip SRAM to off-chip HBM, and use frameworks like the Berkeley Roofline Model to demonstrate how this bottleneck becomes particularly pronounced in Transformer-based LLMs, preventing GPUs from achieving their theoretical compute throughput.

As we push against memory limitations, the typical response is to scale up individual GPUs with more compute and memory. However, this approach eventually hits insurmountable limits due to photolithographic reticle size constraints and the complexities of advanced multi-chip module packaging techniques like CoWoS. These physical constraints force a shift toward scale-out architectures with massive GPU clusters. This transition, however, introduces our second major barrier: the Interconnect Wall. Coordinating computations and exchanging data across thousands of interconnected GPUs creates new bottlenecks, as LLM parallelization strategies demand extreme bandwidth and low latency. We’ll examine how these requirements push interconnect technologies like NVLink and InfiniBand to their physical limits, further constraining our ability to scale efficiently.

The third constraint, the Power Wall, not only drives power consumption toward gigawatt levels in future GPU clusters but also prevents individual GPUs from hitting their theoretical peak in high-intensity LLM workloads. By examining the physics of CMOS technology and mathematical relationship between clock frequency, voltage, and power consumption, we discover why performance gains inevitably lead to large increases in power draw. Because a significant portion of that power turns into heat, TDP (Thermal Design Power) becomes an unyielding limit, prompting solutions like Dynamic Voltage and Frequency Scaling (DVFS) to prevent overheating at the cost of peak GPU performance. Beyond raw FLOPS, we’ll also explore why data movement can consume orders of magnitude more energy than the computations themselves, creating a persistent efficiency bottleneck despite extraordinary GPU power.

By the end of [Part 2](Bits,%20FLOPS,%20and%20Watts_%20A%20Systems-Level%20Perspective%20of%20Scaling%20LLMs.pdf), it will be clear that these three “Walls of Silicon” fundamentally constrain our ambitions to scale LLMs. The path forward demands more than just optimizing our existing GPU hardware — it calls for truly novel compute architectures, new memory technologies, advanced interconnects, or alternative computing paradigms.

[Complete Paper](Bits,%20FLOPS,%20and%20Watts_%20A%20Systems-Level%20Perspective%20of%20Scaling%20LLMs.pdf)